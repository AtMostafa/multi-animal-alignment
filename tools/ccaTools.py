import params
import numpy as np
from matplotlib.ticker import MaxNLocator
import matplotlib as mpl
import pyaldata as pyal
from scipy.stats import wilcoxon
import pandas as pd
from sklearn.decomposition import PCA
import numpy as np
import pandas as pd
from scipy.linalg import qr, svd, inv
from scipy.interpolate import interp1d
import pyaldata as pyal
from sklearn.decomposition import PCA
from scipy.spatial import procrustes
import logging
import torch

from tools import dataTools as dt
from tools import utilityTools as utility
monkey_defs = params.monkey_defs
mouse_defs = params.mouse_defs

def get_ccs(side1df, side2df, epoch, area, n_components, use_procrustes = False): 
    #get data  
    AllData1 = dt.get_data_array(side1df, epoch, area=area, model=n_components)
    AllData2 = dt.get_data_array(side2df, epoch, area=area, model=n_components)
    _,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)

    #get ccs
    ccs=[]
    for sessionData1,sessionData2 in zip(AllData1,AllData2):
        data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,n_components))
        data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,n_components))
        if use_procrustes:
            ccs.append(procrustes_wrapper(data1, data2))
        else:
            ccs.append(canoncorr(data1, data2))
    ccs = np.array(ccs).T 

    return ccs

def get_ccs_upper_bound(side1df, epoch, area, n_components, calc_percentile = True, use_procrustes = False):
    n_iter = params.n_iter * 10

    #get data
    AllData1 = dt.get_data_array(side1df, epoch, area=area, model=n_components)
    n_shared_trial1 = AllData1.shape[2]
    trialList1 = np.arange(n_shared_trial1)

    #get ccs
    CCsU=[]
    for sessionData in AllData1:
        r = []
        for n in range(n_iter):
            params.rng.shuffle(trialList1)
            # non-overlapping randomised trials
            trial1 = trialList1[:n_shared_trial1//2]
            trial2 = trialList1[-(n_shared_trial1//2):]
            data1 = np.reshape(sessionData[:,trial1,:,:], (-1,n_components))
            data2 = np.reshape(sessionData[:,trial2,:,:], (-1,n_components))
            if use_procrustes:
                r.append(procrustes_wrapper(data1, data2))
            else:
                r.append(canoncorr(data1, data2))
        CCsU.append(r)
    CCsU = np.array(CCsU)
    if calc_percentile:
        CCsU = np.percentile(CCsU, 99, axis=1).T

    return CCsU

def get_ccs_lower_bound_monkey(side1df, side2df, area, n_components, len_trial, calc_percentile = True, use_procrustes = False):
    n_iter = params.n_iter * 10

    #get data
    AllData1 = dt._get_data_array(side1df, epoch_L=len_trial, area=area, model=n_components)
    AllData1_ = dt._get_data_array(side2df, epoch_L=len_trial, area=area, model=n_components)
    _,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData1_.shape),axis=0)

    #get ccs
    CCsL=[]
    for sessionData1,sessionData2 in zip(AllData1,AllData1_):
        r = []
        for n in range(n_iter):
            sessionData1_sh = params.rng.permutation(sessionData1,axis=0)
            sessionData2_sh = params.rng.permutation(sessionData2,axis=0)

            data1 = np.reshape(sessionData1_sh[:,:min_trials,:min_time,:], (-1,n_components))
            data2 = np.reshape(sessionData2_sh[:,:min_trials,:min_time,:], (-1,n_components))
            if use_procrustes:
                r.append(procrustes_wrapper(data1, data2))
            else:
                r.append(canoncorr(data1, data2))
        CCsL.append(r)
    CCsL = np.array(CCsL)
    if calc_percentile:
        CCsL = np.percentile(CCsL, 1, axis=1).T
    return CCsL

def get_ccs_lower_bound_mice(side1df, side2df, area, n_components, len_trial, calc_percentile = True, use_procrustes = False):
    n_iter = params.n_iter * 10

    #get data
    AllData1_ = dt.get_data_array(side1df, area=area, model=n_components)
    AllData2_ = dt.get_data_array(side2df, area=area, model=n_components)
    _,_, min_trials, min_time,_ = np.min((AllData1_.shape,AllData2_.shape),axis=0)

    #get ccs
    CCsL=[]
    for sessionData1,sessionData2 in zip(AllData1_,AllData2_):
        r = []
        for n in range(n_iter):
            sessionData1_sh = params.rng.permutation(sessionData1,axis=0)
            sessionData2_sh = params.rng.permutation(sessionData2,axis=0)
            time_idx = params.rng.integers(min_time-len_trial)

            data1 = np.reshape(sessionData1_sh[:,:min_trials,time_idx:time_idx+len_trial,:], (-1,n_components))
            data2 = np.reshape(sessionData2_sh[:,:min_trials,time_idx:time_idx+len_trial,:], (-1,n_components))
            if use_procrustes:
                r.append(procrustes_wrapper(data1, data2))
            else:
                r.append(canoncorr(data1, data2))
        CCsL.append(r)
    CCsL = np.array(CCsL)
    if calc_percentile:
        CCsL = np.percentile(CCsL, 1, axis=1).T
    return CCsL

@utility.report
def plot_cca(ax, ax_hist, allDFs, epoch, area, n_components, dataset = 'monkey', prep=False, use_procrustes = False):

    #get data
    if dataset == 'monkey':
        animals = 'monkeys'
        defs = monkey_defs
        pairFileList = dt.get_paired_files_monkey(allDFs)
        n_animals = len(np.unique([df.monkey[0] for df in allDFs]))
    elif dataset == 'mouse':
        animals = 'mice'
        defs = mouse_defs
        pairFileList = dt.get_paired_files_mouse(allDFs)
        n_animals = len(np.unique([df.mouse[0] for df in allDFs]))
    else:
        raise ValueError('dataset must be monkey or mouse')

    pair_side1df = [allDFs[i] for i,_ in pairFileList]
    pair_side2df = [allDFs[j] for _,j in pairFileList]
    side1df = allDFs

    #get ccs
    if prep:
        len_trial = int(np.round(np.diff(defs.WINDOW_prep)/defs.BIN_SIZE))
    else:
        len_trial = int(np.round(np.diff(defs.WINDOW_exec)/defs.BIN_SIZE))
    allCCs = get_ccs(pair_side1df, pair_side2df, epoch, area, n_components, use_procrustes = use_procrustes)
    if dataset == 'monkey':
        CCsL = get_ccs_lower_bound_monkey(pair_side1df, pair_side2df, area, n_components, len_trial, use_procrustes = use_procrustes)
    else:
        CCsL = get_ccs_lower_bound_mice(pair_side1df, pair_side2df, area, n_components, len_trial, use_procrustes = use_procrustes)
    CCsU = get_ccs_upper_bound(side1df, epoch, area, n_components, use_procrustes=use_procrustes)

    # plotting
    x_ = np.arange(1,n_components+1)
    utility.shaded_errorbar(ax, x_, allCCs, color=params.colors.MainCC, marker = 'o')
    utility.shaded_errorbar(ax, x_, CCsU, color=params.colors.UpperCC, marker = '<', ls='--')
    utility.shaded_errorbar(ax, x_, CCsL, color=params.colors.LowerCC, marker = '>', ls=':')

    ax.set_ylim([-.05,1])
    ax.set_xlim([.6,n_components+.6])
    ax.set_xlabel('Neural mode')
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))
    if use_procrustes:
        ax.set_ylabel('Procrustes correlation')
    else:
        ax.set_ylabel('Canonical correlation')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_bounds([1,n_components])
    ax.spines['left'].set_bounds([0,1])
    ax.text(x=n_components, y=1, s= f'$n={CCsL.shape[1]}$ pairs of sessions\nacross ${n_animals}$ ${animals}$', ha='right', va='top', fontsize=mpl.rcParams['xtick.labelsize'])
    
    #plot the hist
    bins = np.arange(0,1,0.05)
    ax_hist.xaxis.set_visible(False)
    ax_hist.set_facecolor('None')
    ax_hist.spines['bottom'].set_visible(False)
    ax_hist.spines['right'].set_visible(False)
    ax_hist.spines['top'].set_visible(False)
    ax_hist.spines['left'].set_bounds([0,1])
    ax_hist.set_ylim([-.05,1])
    ax_hist.hist(allCCs[:4,:].mean(axis=0), bins=bins, density=True, label=f'Across ($n={allCCs.shape[1]}$)', color=params.colors.MainCC, alpha=.8, orientation='horizontal')
    ax_hist.hist(CCsU[:4,:].mean(axis=0), bins=bins, density=True,label=f'Within ($n={CCsU.shape[1]}$)', color=params.colors.UpperCC, alpha=.8, orientation='horizontal')
    ax_hist.hist(CCsL[:4,:].mean(axis=0), bins=bins, density=True, label=f'Control ($n={CCsL.shape[1]}$)', color=params.colors.LowerCC, alpha=.8, orientation='horizontal')

    ax_hist.tick_params('y', direction='out')
    ax_hist.set_yticklabels([])
    ax_hist.legend(loc=(0,-.05))

    #stats ###################################
    allCCs_median = np.median(allCCs[:4,:].mean(axis=0))
    CCsU_median = np.median(CCsU[:4,:].mean(axis=0))
    CCsL_median = np.median(CCsL[:4,:].mean(axis=0))
    allCCs_mean = allCCs[:4,:].mean(axis=0)
    CCsU_mean = CCsU[:4,:].mean(axis=0)
    CCsL_mean = CCsL[:4,:].mean(axis=0)

    #calc stats
    ##for paired stats
    side1CCsU_mean = [CCsU_mean[i] for i,_ in pairFileList]
    side2CCsU_mean = [CCsU_mean[j] for _,j in pairFileList]
    allCCsU_mean = side1CCsU_mean + side2CCsU_mean
        
    compare_upper_stats = wilcoxon(np.tile(allCCs_mean,2), allCCsU_mean)
    compare_lower_stats = wilcoxon(allCCs_mean, CCsL_mean)

    ##for unpaired stats
    # compare_upper_stats = mannwhitneyu(allCCs_mean, CCsU_mean)
    # compare_lower_stats = mannwhitneyu(allCCs_mean, CCsL_mean)

    print("Across vs within:", compare_upper_stats)
    print("Across vs control:", compare_lower_stats)

    if params.annotate_stats:
        #annotate stats
        xmin, xmax = ax_hist.get_xlim()
        markerx = xmax+(xmax-xmin)*0.05
        linex = xmax+(xmax-xmin)*0.15
        textx = xmax+(xmax-xmin)*0.25
        line_kwargs = dict(linewidth = 0.5, color = 'k')
        text_kwargs = dict(ha='left', va='center')

        ax_hist.scatter(markerx, allCCs_median, color = params.colors.MainCC, marker = '<')
        ax_hist.scatter(markerx, CCsU_median, color = params.colors.UpperCC, marker = '<')
        ax_hist.scatter(markerx, CCsL_median, color = params.colors.LowerCC, marker = '<')

        ax_hist.plot([linex, linex], [allCCs_median, CCsU_median], **line_kwargs)
        ax_hist.plot([linex, linex], [allCCs_median, CCsL_median], linestyle = '--', **line_kwargs)
        
        ax_hist.text(textx, (allCCs_median + CCsU_median)/2, dt.get_signif_annot(compare_upper_stats[1]), **text_kwargs)
        ax_hist.text(textx, (allCCs_median + CCsL_median)/2, dt.get_signif_annot(compare_lower_stats[1]), **text_kwargs)

def plot_cca_for_ex(ax, example_dfs, epoch, area, n_components, dataset = 'monkey', prep = False):

    if dataset == 'monkey':
        animals = 'monkeys'
        defs = monkey_defs
    elif dataset == 'mouse':
        animals = 'mice'
        defs = mouse_defs
    else:
        raise ValueError('dataset must be monkey or mouse')
    
    #get data
    df1, df2 = example_dfs

    #get ccs
    if prep:
        len_trial = int(np.round(np.diff(defs.WINDOW_exec)/defs.BIN_SIZE))
    else:
        len_trial = int(np.round(np.diff(defs.WINDOW_prep)/defs.BIN_SIZE))
    allCCs = get_ccs(df1,df2, epoch, area, n_components)
    if dataset == 'monkey':
        CCsL = get_ccs_lower_bound_monkey(df1,df2, area, n_components, len_trial)
    else:
        CCsL = get_ccs_lower_bound_mice(df1,df2, area, n_components, len_trial)
    CCsU = get_ccs_upper_bound([df1,df2], epoch, area, n_components)

    # plotting
    x_ = np.arange(1,n_components+1)
    ax.plot(x_, allCCs, color=params.colors.MainCC, marker = 'o', label=f'Across ${animals}$')
    ax.plot(x_, CCsU[:,0], color=params.colors.UpperCC, marker = '<', ls='--', label=f'Within 1')
    ax.plot(x_, CCsU[:,1], marker = '<', ls='--', label=f'Within 2')
    ax.plot(x_, CCsL, color=params.colors.LowerCC, marker = '>', ls=':', label=f'Control')

    ax.set_ylim([-.05,1])
    ax.set_xlim([.6, n_components+.6])
    ax.set_xlabel('Neural mode')
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))
    # ax.set_title(f'{defs.areas[2]} Alignment')
    ax.legend(loc=(.55,.67))
    ax.set_ylabel('Canonical correlation')
    
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_bounds([1, n_components])
    ax.spines['left'].set_bounds([0,1])


def canoncorr(X:np.array, Y: np.array, fullReturn: bool = False) -> np.array:
    """
    Canonical Correlation Analysis (CCA)
    line-by-line port from Matlab implementation of `canoncorr`
    X,Y: (samples/observations) x (features) matrix, for both: X.shape[0] >> X.shape[1]
    fullReturn: whether all outputs should be returned or just `r` be returned (not in Matlab)
    
    returns: A,B,r,U,V 
    A,B: Canonical coefficients for X and Y
    U,V: Canonical scores for the variables X and Y
    r:   Canonical correlations
    
    Signature:
    A,B,r,U,V = canoncorr(X, Y)
    """
    n, p1 = X.shape
    p2 = Y.shape[1]
    if p1 >= n or p2 >= n:
        logging.warning('Not enough samples, might cause problems')

    # Center the variables
    X = X - np.mean(X,0);
    Y = Y - np.mean(Y,0);

    # Factor the inputs, and find a full rank set of columns if necessary
    Q1,T11,perm1 = qr(X, mode='economic', pivoting=True, check_finite=True)

    rankX = sum(np.abs(np.diagonal(T11)) > np.finfo(type((np.abs(T11[0,0])))).eps*max([n,p1]));

    if rankX == 0:
        logging.error(f'stats:canoncorr:BadData = X')
    elif rankX < p1:
        logging.warning('stats:canoncorr:NotFullRank = X')
        Q1 = Q1[:,:rankX]
        T11 = T11[rankX,:rankX]

    Q2,T22,perm2 = qr(Y, mode='economic', pivoting=True, check_finite=True)
    rankY = sum(np.abs(np.diagonal(T22)) > np.finfo(type((np.abs(T22[0,0])))).eps*max([n,p2]));

    if rankY == 0:
        logging.error(f'stats:canoncorr:BadData = Y')
    elif rankY < p2:
        logging.warning('stats:canoncorr:NotFullRank = Y')
        Q2 = Q2[:,:rankY];
        T22 = T22[:rankY,:rankY];

    # Compute canonical coefficients and canonical correlations.  For rankX >
    # rankY, the economy-size version ignores the extra columns in L and rows
    # in D. For rankX < rankY, need to ignore extra columns in M and D
    # explicitly. Normalize A and B to give U and V unit variance.
    d = min(rankX,rankY);
    L,D,M = svd(Q1.T @ Q2, full_matrices=True, check_finite=True, lapack_driver='gesdd')
    M = M.T

    A = inv(T11) @ L[:,:d] * np.sqrt(n-1);
    B = inv(T22) @ M[:,:d] * np.sqrt(n-1);
    r = D[:d]
    # remove roundoff errs
    r[r>=1] = 1
    r[r<=0] = 0

    if not fullReturn:
        return r

    # Put coefficients back to their full size and their correct order
    A[perm1,:] = np.vstack((A, np.zeros((p1-rankX,d))))
    B[perm2,:] = np.vstack((B, np.zeros((p2-rankY,d))))
    
    # Compute the canonical variates
    U = X @ A
    V = Y @ B

    return A, B, r, U, V

def canoncorr_torch(X:torch.Tensor, Y: torch.Tensor) -> torch.Tensor:
    """
    Canonical Correlation Analysis (CCA) using torch
    adapted from `canoncorr`, does not do fullReturn
    X,Y: (samples/observations) x (features) matrix, for both: X.shape[0] >> X.shape[1]
    
    r:   Canonical correlations
    
    Signature:
    r = canoncorr(X, Y)
    """
    n, p1 = X.shape
    p2 = Y.shape[1]
    if p1 >= n or p2 >= n:
        logging.warning('Not enough samples, might cause problems')

    #NOTE: removed centering since it did not work with backprop

    # Factor the inputs, and find a full rank set of columns if necessary
    Q1,T11 = torch.linalg.qr(X, mode='reduced')
    Q2,T22 = torch.linalg.qr(Y, mode='reduced')
    rankX = torch.sum(torch.abs(torch.diagonal(T11)) > torch.finfo(torch.abs(T11[0,0]).dtype).eps*max([n,p1]));
    rankY = torch.sum(torch.abs(torch.diagonal(T22)) > torch.finfo(torch.abs(T22[0,0]).dtype).eps*max([n,p1]));
    
    Q1 = Q1[:,:rankX];
    Q2 = Q2[:,:rankY];

    # Canonical correlations
    r = torch.linalg.svdvals(Q1.T @ Q2) 

    return r


def procrustes_wrapper(A: np.ndarray, B: np.ndarray, fullReturn=False):
    """Procrustes alignment wrapper based on `scipy.spatial.procrustes`.
    A, B: (samples/observations) x (features) matrix, for both: A.shape[0] >> A.shape[1]
    fullReturn: whether all outputs should be returned or just `CCs` be returned
    
    returns: U, V, CCs 
    U, V: transformed matrice from A, B
    CCs: Correlations between transfomed signals, equivalent to Canonical correlations
    """
    assert A.shape == B.shape

    U, V, _ = procrustes(A, B)
    CCs = np.array([np.corrcoef(U[:,j],V[:,j])[0,1] for j in range(A.shape[1])])

    if not fullReturn:
        return CCs
    return U, V, CCs

def get_target_id(trial):
    return int(np.round((trial.target_direction + np.pi) / (0.25*np.pi))) - 1

def VAF_pc_cc (X: np.ndarray, C: np.ndarray, A: np.ndarray) -> np.ndarray:
    """
    Calculate Variance Accounted For (VAF) for a double projection (as in from PCA --> to CCA) using the method in Gallego, NatComm, 2018
    
    Parameters
    ----------
    `X`: the data matrix, T x n with _T_ time points and _n_ neurons, and each neuron is **zero mean**.
    
    `C`: the first projection matrix, usually it is the `PCA_model.components_`, but in principle could be any projection matrix with orthogonal bases.
    
    `A` : is the CCA canonical axes, the output of the `canoncorr` function, in principle could be any projection matrix, not necessarily orthogonal.
    
    Returns
    -------
    `VAFs`: np.array with VAF for each axes of `C`, normalised between 0<VAF<1 for each axis, `sum(VAFs)` equals to total VAF.
    """
    # following the notation in Gallego 2018
    D = inv(A.T@A)@A.T@C
    E = C.T@A
    norm = lambda m:np.sum(m**2)
    
    VAFs=np.empty((C.shape[0],))
    for comp in range(1,C.shape[0]+1):
        VAF = norm(X - X @ E[:,:comp] @ D[:comp,:]) / norm(X)
        VAFs[comp-1] = 1-VAF

    VAFs = np.array([VAFs[0],*np.diff(VAFs)])
    return VAFs

def VAF_pyal(df1:pd.DataFrame, field1: str, epoch1,
               df2:pd.DataFrame, field2: str, epoch2,
               n_components:int =10) -> (np.ndarray, np.ndarray):
    """
    Measure VAF for each CCA axis, between 2 DataFrames, fields, time epochs.
    epoch1, epoch2: an instance of the `pyal.generate_epoch_fun` function.
    """
 
    df1 = pyal.restrict_to_interval(df1,epoch_fun=epoch1)
    rates_1 = np.concatenate(df1[field1].values, axis=0)
    rates_1_model = PCA(n_components=n_components, svd_solver='full').fit(rates_1)
    rates_1_C = rates_1_model.components_
    df1 = pyal.apply_dim_reduce_model(df1, rates_1_model, field1, '_pca');
    pca_1_data = np.concatenate(df1['_pca'].values, axis=0)

    
    df2 = pyal.restrict_to_interval(df2, epoch_fun=epoch2)
    rates_2 = np.concatenate(df2[field2].values, axis=0)
    rates_2_model = PCA(n_components=n_components, svd_solver='full').fit(rates_2)
    rates_2_C = rates_2_model.components_
    df2 = pyal.apply_dim_reduce_model(df2, rates_2_model, field2, '_pca');
    pca_2_data = np.concatenate(df2['_pca'].values, axis=0)
    
    
    # same number of timepoints in both matrices
    n_samples = min ([pca_1_data.shape[0], pca_2_data.shape[0]])
    pca_1_data = pca_1_data[:n_samples,:]
    pca_2_data = pca_2_data[:n_samples,:]

    A, B, *_ = canoncorr(pca_1_data, pca_2_data, fullReturn=True)
    VAFs1 = VAF_pc_cc(rates_1, rates_1_C, A)
    VAFs2 = VAF_pc_cc(rates_2, rates_2_C, B)
    
    return VAFs1, VAFs2

def VAF_pc_cc_pyal(df1:pd.DataFrame, field1: str, epoch1, target1: int,
                   df2:pd.DataFrame, field2: str, epoch2, target2: int,
                   n_components:int =10) -> (np.ndarray, np.ndarray):
    """
    Measure VAF for each CCA axis, between 2 DataFrames, fields, time epochs, and targets.
    epoch1, epoch2: an instance of the `pyal.generate_epoch_fun` function.
    """
    if "target_id" not in df1.columns:
        df1["target_id"] = df1.apply(get_target_id, axis=1)
    if "target_id" not in df2.columns:
        df2["target_id"] = df2.apply(get_target_id, axis=1)
 
    df1 = pyal.restrict_to_interval(df1,epoch_fun=epoch1)
    rates_1 = np.concatenate(df1[field1].values, axis=0)
    rates_1_model = PCA(n_components=n_components, svd_solver='full').fit(rates_1)
    rates_1_C = rates_1_model.components_
    df1 = pyal.apply_dim_reduce_model(df1, rates_1_model, field1, '_pca');

    
    df1 = pyal.select_trials(df1, df1.target_id==target1)
    pca_1_target = np.concatenate(df1['_pca'].values, axis=0)

    
    df2 = pyal.restrict_to_interval(df2, epoch_fun=epoch2)
    rates_2 = np.concatenate(df2[field2].values, axis=0)
    rates_2_model = PCA(n_components=n_components, svd_solver='full').fit(rates_2)
    rates_2_C = rates_2_model.components_
    df2 = pyal.apply_dim_reduce_model(df2, rates_2_model, field2, '_pca');
    
    df2 = pyal.select_trials(df2, df2.target_id==target2)
    pca_2_target = np.concatenate(df2['_pca'].values, axis=0)
    
    
    # same number of timepoints in both matrices
    n_samples = min ([pca_1_target.shape[0], pca_2_target.shape[0]])
    pca_1_target = pca_1_target[:n_samples,:]
    pca_2_target = pca_2_target[:n_samples,:]

    A, B, r, _, _ = canoncorr(pca_1_target, pca_2_target, fullReturn=True)
    VAFs1 = VAF_pc_cc(rates_1, rates_1_C, A)
    VAFs2 = VAF_pc_cc(rates_2, rates_2_C, B)
    
    return VAFs1, VAFs2, r

def VAF_pc_cc_pyal2(df1:pd.DataFrame, field1: str, epoch1, target1: int,
                    df2:pd.DataFrame, field2: str, epoch2, target2: int,
                    n_components:int =10, n_iter:int =20):
    """
    Identical to `VAF_pc_cc_pyal`, ...
    except that it  tries to correct for _very_ different number of units by...
    subsampling the larger population `n_iter` times and averaging over the results.
    """
    if "target_id" not in df1.columns:
        df1["target_id"] = df1.apply(get_target_id, axis=1)
    if "target_id" not in df2.columns:
        df2["target_id"] = df2.apply(get_target_id, axis=1)
 
    df1_ = pyal.restrict_to_interval(df1,epoch_fun=epoch1)
    rates_1 = np.concatenate(df1_[field1].values, axis=0)

    df2_ = pyal.restrict_to_interval(df2, epoch_fun=epoch2)
    rates_2 = np.concatenate(df2_[field2].values, axis=0)
    
    # PCA
    ## check for `n`
    n1 = rates_1.shape[1]
    n2 = rates_2.shape[1]
    n_s, n_l = min(n1, n2), max(n1, n2)
    
    if n1 >= n2:
        array1Bigger = True
    else:
        array1Bigger = False
    diffTooBig = (abs(n1-n2)/min(n1,n2) >= 2) or (abs(n1-n2) > 100)  # boolean

    rng = np.random.default_rng(12345)
    VAFs1, VAFs2, R = [], [], []
    if diffTooBig:
#         print(f'correcting for numbesr of units: {n1, n2}')
        for i in range(n_iter):
            idx = rng.choice(n_l, n_s)
            array_new = rates_1 if array1Bigger else rates_2
            array_new = array_new[:,idx]
            array_new_model = PCA(n_components=n_components, svd_solver='full').fit(array_new)
            
            if array1Bigger:
                PCA_model = PCA(n_components=n_components, svd_solver='full').fit(rates_2)
            else:
                PCA_model = PCA(n_components=n_components, svd_solver='full').fit(rates_1)

            rates_1_model = array_new_model if array1Bigger else PCA_model
            rates_1_C = rates_1_model.components_
            df1__ = pyal.select_trials(df1_, df1_.target_id==target1)
            rates_1_target = np.concatenate(df1__[field1].values, axis=0)
            pca_1_target = rates_1_model.transform(rates_1_target[:,idx]) if array1Bigger else rates_1_model.transform(rates_1_target)

            rates_2_model = PCA_model if array1Bigger else array_new_model
            rates_2_C = rates_2_model.components_
            df2__ = pyal.select_trials(df2_, df2_.target_id==target2)
            rates_2_target = np.concatenate(df2__[field2].values, axis=0)
            pca_2_target = rates_2_model.transform(rates_2_target) if array1Bigger else rates_2_model.transform(rates_2_target[:,idx])

            # same number of timepoints in both matrices
            n_samples = min ([pca_1_target.shape[0], pca_2_target.shape[0]])
            pca_1_target = pca_1_target[:n_samples,:]
            pca_2_target = pca_2_target[:n_samples,:]

            A, B, r, _, _ = canoncorr(pca_1_target, pca_2_target, fullReturn=True)
            V1 = VAF_pc_cc(array_new if array1Bigger else rates_1, rates_1_C, A)
            V2 = VAF_pc_cc(rates_2 if array1Bigger else array_new, rates_2_C, B)
            VAFs1.append(V1)
            VAFs2.append(V2)
            R.append(r)
            
        VAFs1 = np.mean(np.array(VAFs1), axis=0)
        VAFs2 = np.mean(np.array(VAFs2), axis=0)
        R = np.mean(np.array(R), axis=0)
    else:
        VAFs1,VAFs2,R = VAF_pc_cc_pyal(df1, field1, epoch1, target1,
                                       df2, field2, epoch2, target2, n_components)

    return VAFs1, VAFs2, R

